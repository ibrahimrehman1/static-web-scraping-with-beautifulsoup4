{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "URL = \"http://www.wikipedia.org\"\n",
    "response = requests.get(URL)\n",
    "print(response.content)\n",
    "print(response.text)\n",
    "print(response.status_code)\n",
    "response.raise_for_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "print(soup.prettify())\n",
    "\n",
    "print(soup.title)\n",
    "print(soup.title.text)\n",
    "print(soup.title.get_text()[0::1])\n",
    "\n",
    "print(soup.h1)\n",
    "print(soup.h1.parent.name)\n",
    "\n",
    "print(soup.body.h1.previous_sibling)\n",
    "print(soup.body.h1.attrs['class'][0])\n",
    "\n",
    "print(soup.find_all(\"a\"))\n",
    "print(soup.find(class_=\"link-box\"))\n",
    "print(list(soup.body.div.stripped_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def returnMatches(target):\n",
    "    return re.findall(\"[a-zA-Z]+\", target)\n",
    "\n",
    "\n",
    "print(returnMatches(soup.body.div.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select(\"h1 > span\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "filename = \"wikipedia-scraped-data.pickle\"\n",
    "if os.path.exists(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        response = pickle.load(f)\n",
    "\n",
    "else: \n",
    "    result = requests.get(URL)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(result, f)\n",
    "\n",
    "\n",
    "print(result.text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Community portal', 'Village pump', 'Site news', 'Teahouse', 'Help desk', 'Reference desk', 'Content portals']\n",
      "['The central hub for editors, with resources, links, tasks, and announcements.', 'Forum for discussions about Wikipedia itself, including policies and technical issues.', 'Sources of news about Wikipedia and the broader Wikimedia movement.', 'Ask basic questions about using or editing Wikipedia.', 'Ask questions about using or editing Wikipedia.', 'Ask research questions about encyclopedic topics.', 'A unique way to navigate the encyclopedia.']\n",
      "[{'Links': 'Community portal', 'Links Description': 'The central hub for editors, with resources, links, tasks, and announcements.'}, {'Links': 'Village pump', 'Links Description': 'Forum for discussions about Wikipedia itself, including policies and technical issues.'}, {'Links': 'Site news', 'Links Description': 'Sources of news about Wikipedia and the broader Wikimedia movement.'}, {'Links': 'Teahouse', 'Links Description': 'Ask basic questions about using or editing Wikipedia.'}, {'Links': 'Help desk', 'Links Description': 'Ask questions about using or editing Wikipedia.'}, {'Links': 'Reference desk', 'Links Description': 'Ask research questions about encyclopedic topics.'}, {'Links': 'Content portals', 'Links Description': 'A unique way to navigate the encyclopedia.'}]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# row = [{\"First Name\": \"Ibrahim\", \"Last Name\": \"Rehman\", \"Age\": 20}, {\"First Name\": \"Usman\", \"Last Name\": \"Naveed\", \"Age\": 20}, {\"First Name\": \"Fasihullah\", \"Last Name\": \"Ejaz\", \"Age\": 20}]\n",
    "\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "links = soup.select(\"#mp-other-content > ul > li a\")\n",
    "linksArr = []\n",
    "for anchors in links:\n",
    "    linksArr.append(anchors.text)\n",
    "linksDes = soup.select(\"#mp-other-content > ul > li\")\n",
    "linksDesArr = []\n",
    "for linkDes in linksDes:\n",
    "    linksDesArr.append(linkDes.text.split(\"â€“ \")[-1])\n",
    "\n",
    "\n",
    "rowHead = [\"Links\", \"Links Description\"]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in range(len(linksArr)):\n",
    "    rows.append({\"Links\": linksArr[i], \"Links Description\": linksDesArr[i]})\n",
    "\n",
    "\n",
    "with open(\"wikipedia-scraped-data.csv\", \"w\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=rowHead)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9944e8c1532b517148f83aa918c800bb59db73773167f3371cd344c3140d0461"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
