{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "URL = \"http://www.wikipedia.org\"\n",
    "response = requests.get(URL)\n",
    "print(response.content)\n",
    "print(response.text)\n",
    "print(response.status_code)\n",
    "response.raise_for_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "print(soup.prettify())\n",
    "\n",
    "print(soup.title)\n",
    "print(soup.title.text)\n",
    "print(soup.title.get_text()[0::1])\n",
    "\n",
    "print(soup.h1)\n",
    "print(soup.h1.parent.name)\n",
    "\n",
    "print(soup.body.h1.previous_sibling)\n",
    "print(soup.body.h1.attrs['class'][0])\n",
    "\n",
    "print(soup.find_all(\"a\"))\n",
    "print(soup.find(class_=\"link-box\"))\n",
    "print(list(soup.body.div.stripped_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def returnMatches(target):\n",
    "    return re.findall(\"[a-zA-Z]+\", target)\n",
    "\n",
    "\n",
    "print(returnMatches(soup.body.div.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select(\"h1 > span\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "filename = \"wikipedia-scraped-data.pickle\"\n",
    "if os.path.exists(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        response = pickle.load(f)\n",
    "\n",
    "else: \n",
    "    result = requests.get(URL)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(result, f)\n",
    "\n",
    "\n",
    "print(result.text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# row = [{\"First Name\": \"Ibrahim\", \"Last Name\": \"Rehman\", \"Age\": 20}, {\"First Name\": \"Usman\", \"Last Name\": \"Naveed\", \"Age\": 20}, {\"First Name\": \"Fasihullah\", \"Last Name\": \"Ejaz\", \"Age\": 20}]\n",
    "\n",
    "\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "links = soup.select(\"#mp-other-content > ul > li a\")\n",
    "linksArr = []\n",
    "for anchors in links:\n",
    "    linksArr.append(anchors.text)\n",
    "linksDes = soup.select(\"#mp-other-content > ul > li\")\n",
    "linksDesArr = []\n",
    "for linkDes in linksDes:\n",
    "    linksDesArr.append(linkDes.text.split(\"â€“ \")[-1])\n",
    "\n",
    "\n",
    "rowHead = [\"Links\", \"Links Description\"]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in range(len(linksArr)):\n",
    "    rows.append({\"Links\": linksArr[i], \"Links Description\": linksDesArr[i]})\n",
    "\n",
    "\n",
    "with open(\"wikipedia-scraped-data.csv\", \"w\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=rowHead)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_csv(\"wikipedia-scraped-data.csv\")\n",
    "df = df_raw.copy()\n",
    "# df.head()\n",
    "# df.shape\n",
    "# df.sample(2)\n",
    "# df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.links.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Links.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.countplot(data=df, y='Links Description')\n",
    "ax.set_title(\"Count Plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query(\"Links != 'Community portal' and Links != 'Village pump'\").copy()\n",
    "df.head()\n",
    "\n",
    "df['Links Author'] = df.Links.apply(lambda link: f\"{len(link)} Links\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.Links.hist()\n",
    "ax.set_xlabel(\"Links\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Links Histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Links.str.upper()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9944e8c1532b517148f83aa918c800bb59db73773167f3371cd344c3140d0461"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
